{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to c:\\users\\bitcamp\\appdata\\local\\temp\\pip-req-build-ei6hpplf\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit e1f2f37055e7460d8427f6912579c0162cb69831\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py): started\n",
      "  Building wheel for kobert (setup.py): finished with status 'done'\n",
      "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15758 sha256=91dd1252e0c1e540415f33f7c6cecfbf9b4b8fe9daf9f32146d4da356962c26e\n",
      "  Stored in directory: C:\\Users\\bitcamp\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ie9jlywx\\wheels\\bf\\5f\\74\\81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n",
      "Successfully built kobert\n",
      "Installing collected packages: kobert\n",
      "  Attempting uninstall: kobert\n",
      "    Found existing installation: kobert 0.2.3\n",
      "    Uninstalling kobert-0.2.3:\n",
      "      Successfully uninstalled kobert-0.2.3\n",
      "Successfully installed kobert-0.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' 'C:\\Users\\bitcamp\\AppData\\Local\\Temp\\pip-req-build-ei6hpplf'\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-deps --force-reinstall git+https://git@github.com/SKTBrain/KoBERT.git@master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PROTOCOL_TLS' from 'urllib3.util.ssl_' (c:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\urllib3\\util\\ssl_.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32mc:\\Users\\bitcamp\\EI\\WJ-jupyter\\nlp_example\\kobert_test.ipynb 셀 3\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bitcamp/EI/WJ-jupyter/nlp_example/kobert_test.ipynb#ch0000002?line=0'>1</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkobert\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m get_tokenizer\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bitcamp/EI/WJ-jupyter/nlp_example/kobert_test.ipynb#ch0000002?line=1'>2</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkobert\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mpytorch_kobert\u001B[39;00m \u001B[39mimport\u001B[39;00m get_pytorch_kobert_model\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\kobert\\__init__.py:16\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# coding=utf-8\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39m# Copyright 2019 SK T-Brain Authors.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[39m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[39m# limitations under the License.\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkobert\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m download, get_tokenizer\n\u001B[0;32m     17\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkobert\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mpytorch_kobert\u001B[39;00m \u001B[39mimport\u001B[39;00m get_pytorch_kobert_model\n\u001B[0;32m     18\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkobert\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mmxnet_kobert\u001B[39;00m \u001B[39mimport\u001B[39;00m get_mxnet_kobert_model\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\kobert\\utils\\__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkobert\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m download, get_tokenizer\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\kobert\\utils\\utils.py:19\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mhashlib\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mos\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mkobert\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39maws_s3_downloader\u001B[39;00m \u001B[39mimport\u001B[39;00m AwsS3Downloader\n\u001B[0;32m     22\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mdownload\u001B[39m(url, chksum\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m, cachedir\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m.cache\u001B[39m\u001B[39m\"\u001B[39m):\n\u001B[0;32m     23\u001B[0m     cachedir_full \u001B[39m=\u001B[39m os\u001B[39m.\u001B[39mpath\u001B[39m.\u001B[39mjoin(os\u001B[39m.\u001B[39mgetcwd(), cachedir)\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\kobert\\utils\\aws_s3_downloader.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mboto3\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mos\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39msys\u001B[39;00m\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\boto3\\__init__.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mlogging\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mboto3\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcompat\u001B[39;00m \u001B[39mimport\u001B[39;00m _warn_deprecated_python\n\u001B[1;32m---> 17\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mboto3\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39msession\u001B[39;00m \u001B[39mimport\u001B[39;00m Session\n\u001B[0;32m     19\u001B[0m __author__ \u001B[39m=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mAmazon Web Services\u001B[39m\u001B[39m'\u001B[39m\n\u001B[0;32m     20\u001B[0m __version__ \u001B[39m=\u001B[39m \u001B[39m'\u001B[39m\u001B[39m1.24.29\u001B[39m\u001B[39m'\u001B[39m\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\boto3\\session.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mcopy\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mos\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39msession\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mclient\u001B[39;00m \u001B[39mimport\u001B[39;00m Config\n\u001B[0;32m     19\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mexceptions\u001B[39;00m \u001B[39mimport\u001B[39;00m DataNotFoundError, UnknownServiceError\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\session.py:26\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39msocket\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mwarnings\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mclient\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mconfigloader\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcredentials\u001B[39;00m\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\client.py:15\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[39m# ANY KIND, either express or implied. See the License for the specific\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[39m# language governing permissions and limitations under the License.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mlogging\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m \u001B[39mimport\u001B[39;00m waiter, xform_name\n\u001B[0;32m     16\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39margs\u001B[39;00m \u001B[39mimport\u001B[39;00m ClientArgsCreator\n\u001B[0;32m     17\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mauth\u001B[39;00m \u001B[39mimport\u001B[39;00m AUTH_TYPE_MAPS\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\waiter.py:18\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mtime\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mjmespath\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocstring\u001B[39;00m \u001B[39mimport\u001B[39;00m WaiterDocstring\n\u001B[0;32m     19\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m get_service_module_name\n\u001B[0;32m     21\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39m.\u001B[39;00m \u001B[39mimport\u001B[39;00m xform_name\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\docs\\__init__.py:15\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[39m# ANY KIND, either express or implied. See the License for the specific\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[39m# language governing permissions and limitations under the License.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mos\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mservice\u001B[39;00m \u001B[39mimport\u001B[39;00m ServiceDocumenter\n\u001B[0;32m     18\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mgenerate_docs\u001B[39m(root_dir, session):\n\u001B[0;32m     19\u001B[0m     \u001B[39m\"\"\"Generates the reference documentation for botocore\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \n\u001B[0;32m     21\u001B[0m \u001B[39m    This will go through every available AWS service and output ReSTructured\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[39m        root_dir/reference/services/service-name.rst\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\docs\\service.py:14\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[39m# ANY KIND, either express or implied. See the License for the specific\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[39m# language governing permissions and limitations under the License.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mbcdoc\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mrestdoc\u001B[39;00m \u001B[39mimport\u001B[39;00m DocumentStructure\n\u001B[1;32m---> 14\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mclient\u001B[39;00m \u001B[39mimport\u001B[39;00m ClientDocumenter, ClientExceptionsDocumenter\n\u001B[0;32m     15\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mpaginator\u001B[39;00m \u001B[39mimport\u001B[39;00m PaginatorDocumenter\n\u001B[0;32m     16\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mwaiter\u001B[39;00m \u001B[39mimport\u001B[39;00m WaiterDocumenter\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\docs\\client.py:14\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[39m# ANY KIND, either express or implied. See the License for the specific\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[39m# language governing permissions and limitations under the License.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcompat\u001B[39;00m \u001B[39mimport\u001B[39;00m OrderedDict\n\u001B[1;32m---> 14\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mexample\u001B[39;00m \u001B[39mimport\u001B[39;00m ResponseExampleDocumenter\n\u001B[0;32m     15\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mmethod\u001B[39;00m \u001B[39mimport\u001B[39;00m (\n\u001B[0;32m     16\u001B[0m     document_custom_method,\n\u001B[0;32m     17\u001B[0m     document_model_driven_method,\n\u001B[0;32m     18\u001B[0m     get_instance_public_methods,\n\u001B[0;32m     19\u001B[0m )\n\u001B[0;32m     20\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mparams\u001B[39;00m \u001B[39mimport\u001B[39;00m ResponseParamsDocumenter\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\docs\\example.py:13\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[39m# ANY KIND, either express or implied. See the License for the specific\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[39m# language governing permissions and limitations under the License.\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mshape\u001B[39;00m \u001B[39mimport\u001B[39;00m ShapeDocumenter\n\u001B[0;32m     14\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mdocs\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m py_default\n\u001B[0;32m     17\u001B[0m \u001B[39mclass\u001B[39;00m \u001B[39mBaseExampleDocumenter\u001B[39;00m(ShapeDocumenter):\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\docs\\shape.py:19\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[39m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[39m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[39m# inherited from a Documenter class with the appropriate methods\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[39m# and attributes.\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutils\u001B[39;00m \u001B[39mimport\u001B[39;00m is_json_value_header\n\u001B[0;32m     22\u001B[0m \u001B[39mclass\u001B[39;00m \u001B[39mShapeDocumenter\u001B[39;00m:\n\u001B[0;32m     23\u001B[0m     EVENT_NAME \u001B[39m=\u001B[39m \u001B[39m'\u001B[39m\u001B[39m'\u001B[39m\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\utils.py:34\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mawsrequest\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m \u001B[39mimport\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mhttpsession\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[39m# IP Regexes retained for backwards compatibility\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mbotocore\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mcompat\u001B[39;00m \u001B[39mimport\u001B[39;00m HEX_PAT  \u001B[39m# noqa: F401\u001B[39;00m\n",
      "File \u001B[1;32mc:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\botocore\\httpsession.py:21\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39murllib3\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mexceptions\u001B[39;00m \u001B[39mimport\u001B[39;00m SSLError \u001B[39mas\u001B[39;00m URLLib3SSLError\n\u001B[0;32m     20\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39murllib3\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutil\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mretry\u001B[39;00m \u001B[39mimport\u001B[39;00m Retry\n\u001B[1;32m---> 21\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39murllib3\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutil\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mssl_\u001B[39;00m \u001B[39mimport\u001B[39;00m (\n\u001B[0;32m     22\u001B[0m     DEFAULT_CIPHERS,\n\u001B[0;32m     23\u001B[0m     OP_NO_COMPRESSION,\n\u001B[0;32m     24\u001B[0m     PROTOCOL_TLS,\n\u001B[0;32m     25\u001B[0m     OP_NO_SSLv2,\n\u001B[0;32m     26\u001B[0m     OP_NO_SSLv3,\n\u001B[0;32m     27\u001B[0m     is_ipaddress,\n\u001B[0;32m     28\u001B[0m     ssl,\n\u001B[0;32m     29\u001B[0m )\n\u001B[0;32m     30\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39murllib3\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mutil\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39murl\u001B[39;00m \u001B[39mimport\u001B[39;00m parse_url\n\u001B[0;32m     32\u001B[0m \u001B[39mtry\u001B[39;00m:\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'PROTOCOL_TLS' from 'urllib3.util.ssl_' (c:\\ProgramData\\Anaconda3\\envs\\mibot\\lib\\site-packages\\urllib3\\util\\ssl_.py)"
     ]
    }
   ],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.7\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "print(urllib3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#GPU 사용\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. c:\\Users\\bitcamp\\EI\\WJ-jupyter\\nlp_example\\.cache\\kobert_v1.zip\n",
      "using cached model. c:\\Users\\bitcamp\\EI\\WJ-jupyter\\nlp_example\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chatbot_data = pd.read_excel('./data/한국어_단발성_대화_데이터셋.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>공포</th>\n",
       "      <th>5468</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36627</th>\n",
       "      <td>아이고 박사모님들 어쩌나</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34690</th>\n",
       "      <td>결국 지는 안 간다는 뜻이네?</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29718</th>\n",
       "      <td>나지완! 난 너랑 함께할께^^ 모두 널 주목하지않고 있지만 움츠려들지말고 어깨펴!!...</td>\n",
       "      <td>행복</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22347</th>\n",
       "      <td>인정도 안하는 일본 전 대사관과 영사관앞에 설치하게 정부에서 조용히 독려하는게 맞다.</td>\n",
       "      <td>중립</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29113</th>\n",
       "      <td>같은 여자로써 너무 멋지십니다.</td>\n",
       "      <td>행복</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32744</th>\n",
       "      <td>100만이 모였음에도 뒷처리까지 깔끔하게 시민의식 보여준 국민들 대단합니다!!</td>\n",
       "      <td>행복</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37718</th>\n",
       "      <td>황교안 개 노잼 틀딱충이라 저렇게 못논다 기대할걸 기대하셈~</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15248</th>\n",
       "      <td>아예 국립현충원에서 해라 씹빡들아</td>\n",
       "      <td>분노</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31033</th>\n",
       "      <td>당신은 구단선정한 선수이기보다팬들이선정한 미스터엘지임다.....</td>\n",
       "      <td>행복</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>그래도 기본적으로 화가가 직접 그려야지~~~</td>\n",
       "      <td>놀람</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Emotion  Unnamed: 2  \\\n",
       "36627                                      아이고 박사모님들 어쩌나      혐오         NaN   \n",
       "34690                                   결국 지는 안 간다는 뜻이네?      혐오         NaN   \n",
       "29718  나지완! 난 너랑 함께할께^^ 모두 널 주목하지않고 있지만 움츠려들지말고 어깨펴!!...      행복         NaN   \n",
       "22347    인정도 안하는 일본 전 대사관과 영사관앞에 설치하게 정부에서 조용히 독려하는게 맞다.      중립         NaN   \n",
       "29113                                  같은 여자로써 너무 멋지십니다.      행복         NaN   \n",
       "32744        100만이 모였음에도 뒷처리까지 깔끔하게 시민의식 보여준 국민들 대단합니다!!      행복         NaN   \n",
       "37718                  황교안 개 노잼 틀딱충이라 저렇게 못논다 기대할걸 기대하셈~      혐오         NaN   \n",
       "15248                                 아예 국립현충원에서 해라 씹빡들아      분노         NaN   \n",
       "31033                당신은 구단선정한 선수이기보다팬들이선정한 미스터엘지임다.....      행복         NaN   \n",
       "9089                            그래도 기본적으로 화가가 직접 그려야지~~~      놀람         NaN   \n",
       "\n",
       "       Unnamed: 3  Unnamed: 4   공포  5468  \n",
       "36627         NaN         NaN  NaN   NaN  \n",
       "34690         NaN         NaN  NaN   NaN  \n",
       "29718         NaN         NaN  NaN   NaN  \n",
       "22347         NaN         NaN  NaN   NaN  \n",
       "29113         NaN         NaN  NaN   NaN  \n",
       "32744         NaN         NaN  NaN   NaN  \n",
       "37718         NaN         NaN  NaN   NaN  \n",
       "15248         NaN         NaN  NaN   NaN  \n",
       "31033         NaN         NaN  NaN   NaN  \n",
       "9089          NaN         NaN  NaN   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6\n",
    "\n",
    "data_list = []\n",
    "for q, label in zip(chatbot_data['Sentence'], chatbot_data['Emotion'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['언니 동생으로 부르는게 맞는 일인가요..??', '0']\n",
      "['기술적으로도 아직도 해체해서 다시 완벽히 돌려놓는게 어려운데 해체를한다고?', '1']\n",
      "['당연히 그렇게 해야지 우리나라도 판매를 중단하라', '2']\n",
      "['그거들은 뒤부터 미치겠어요...', '3']\n",
      "['최악의 상황중 그나마 나은 방법이네. 기분은 잡치겠지만', '4']\n",
      "['  요리하는것이 숙제하는것처럼 힘든저에게 용기나게 해주시고 할수 있을것같은 희망을 주셔서감사합니다!!', '5']\n",
      "['와이프도 그렇고 댓글 다 볼텐데 이휘재 좀 하차 하라고 전해주세요', '6']\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])\n",
    "print(data_list[6000])\n",
    "print(data_list[12000])\n",
    "print(data_list[18000])\n",
    "print(data_list[24000])\n",
    "print(data_list[30000])\n",
    "print(data_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#train & test 데이터로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28945\n",
      "9649\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 10\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. c:\\Users\\bitcamp\\EI\\WJ-jupyter\\nlp_example\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 1189,  517, 6188, 7245, 7063,  517,  463, 3486, 7836, 5966,\n",
       "        1698,  517, 6188, 7245, 7063,  517,  463, 1281, 7870, 1801, 6885,\n",
       "        7088, 5966, 1698, 5837, 5837,  517, 6188, 7245, 6398, 6037, 7063,\n",
       "         517,  463,  517,  463,  517,  364,  517,  364,    3,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " array(42),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=7,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2283ea5a520>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36e0ad61291452db0cbf000841c98a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mc:\\Users\\bitcamp\\EI\\WJ-jupyter\\nlp_example\\kobert_test.ipynb 셀 22\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bitcamp/EI/WJ-jupyter/nlp_example/kobert_test.ipynb#ch0000021?line=0'>1</a>\u001B[0m tokenizer \u001B[39m=\u001B[39m get_tokenizer()\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bitcamp/EI/WJ-jupyter/nlp_example/kobert_test.ipynb#ch0000021?line=1'>2</a>\u001B[0m tok \u001B[39m=\u001B[39m nlp\u001B[39m.\u001B[39mdata\u001B[39m.\u001B[39mBERTSPTokenizer(tokenizer, vocab, lower\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/bitcamp/EI/WJ-jupyter/nlp_example/kobert_test.ipynb#ch0000021?line=3'>4</a>\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mpredict\u001B[39m(predict_sentence):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'get_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0:\n",
    "                test_eval.append(\"공포가\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                test_eval.append(\"놀람이\")\n",
    "            elif np.argmax(logits) == 2:\n",
    "                test_eval.append(\"분노가\")\n",
    "            elif np.argmax(logits) == 3:\n",
    "                test_eval.append(\"슬픔이\")\n",
    "            elif np.argmax(logits) == 4:\n",
    "                test_eval.append(\"중립이\")\n",
    "            elif np.argmax(logits) == 5:\n",
    "                test_eval.append(\"행복이\")\n",
    "            elif np.argmax(logits) == 6:\n",
    "                test_eval.append(\"혐오가\")\n",
    "\n",
    "        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mibot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e3d47cc14d0a7d841567c666b1bce259a39d7b78ffe0f6911e5c2094b850179"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}